{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evaluate_RCNN_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "89fa3ee5682e45c6b6defee40e4707a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9a27abdfe9e24f4895d1cff8a19e3987",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c73a8b430d6a432f8574f187704d9bed",
              "IPY_MODEL_e045d0bee8aa4c6b8528b815ddb4ff34"
            ]
          }
        },
        "9a27abdfe9e24f4895d1cff8a19e3987": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c73a8b430d6a432f8574f187704d9bed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_704f7ac3160349c4b9fec40b02111279",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 178090079,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 178090079,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dd9497bf1d8445eb8468e5f53d49e664"
          }
        },
        "e045d0bee8aa4c6b8528b815ddb4ff34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_649f4be9e3a944e3acfbed15b4c84a54",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170M/170M [00:04&lt;00:00, 36.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fb25c12bae544d4399a81858e93c74c7"
          }
        },
        "704f7ac3160349c4b9fec40b02111279": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dd9497bf1d8445eb8468e5f53d49e664": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "649f4be9e3a944e3acfbed15b4c84a54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fb25c12bae544d4399a81858e93c74c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkfwees/cs230_2020/blob/master/evaluate_RCNN_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaqNPUD0IpU_",
        "colab_type": "code",
        "outputId": "5d2ad495-0261-4206-8097-521e2a9aa2f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "pip install cython\n",
        "# Install pycocotools, the version by default in Colab\n",
        "# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n",
        "pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (0.29.19)\n",
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-_p06j9cy\n",
            "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-_p06j9cy\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (47.1.1)\n",
            "Requirement already satisfied, skipping upgrade: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.19)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.18.4)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools==2.0) (1.12.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=267010 sha256=62beb02bc640a02a5582de0fec49499ae2cc36e82aa29fe93d5b7ae094ccc91d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lof5jwfm/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3a\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "  Found existing installation: pycocotools 2.0.0\n",
            "    Uninstalling pycocotools-2.0.0:\n",
            "      Successfully uninstalled pycocotools-2.0.0\n",
            "Successfully installed pycocotools-2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aduglFOwIupE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get all needed resources\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taDWgpVYIw6k",
        "colab_type": "code",
        "outputId": "2a57009d-36d8-4675-f93d-500dd81389a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# mount data to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpS6kfozJtkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SatelliteDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root,  \"/content/drive/My Drive/CS230/data/bpop150k/unlabeled\"))))\n",
        "        self.imgs += list(sorted(os.listdir(os.path.join(root,  \"/content/drive/My Drive/CS230/data/brural/unlabeled\"))))\n",
        "        self.imgs += list(sorted(os.listdir(os.path.join(root,  \"/content/drive/My Drive/CS230/data/bpop1000k/unlabeled\"))))\n",
        "        self.imgs += list(sorted(os.listdir(os.path.join(root,  \"/content/drive/My Drive/CS230/data/bpopunlmtd/unlabeled\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"/content/drive/My Drive/CS230/data/bpop150k/masks\"))))\n",
        "        self.masks += list(sorted(os.listdir(os.path.join(root, \"/content/drive/My Drive/CS230/data/brural/masks\"))))\n",
        "        self.masks += list(sorted(os.listdir(os.path.join(root, \"/content/drive/My Drive/CS230/data/bpop1000k/masks\"))))\n",
        "        self.masks += list(sorted(os.listdir(os.path.join(root, \"/content/drive/My Drive/CS230/data/bpopunlmtd/masks\"))))\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and masks\n",
        "\n",
        "        if self.imgs[idx].__contains__('bpop150k'):\n",
        "          img_path = os.path.join(self.root,  \"/content/drive/My Drive/CS230/data/bpop150k/unlabeled\", self.imgs[idx])\n",
        "          mask_path = os.path.join(self.root, \"/content/drive/My Drive/CS230/data/bpop150k/masks\", self.masks[idx])\n",
        "        elif self.imgs[idx].__contains__('brural'):\n",
        "          img_path = os.path.join(self.root,  \"/content/drive/My Drive/CS230/data/brural/unlabeled\", self.imgs[idx])\n",
        "          mask_path = os.path.join(self.root, \"/content/drive/My Drive/CS230/data/brural/masks\", self.masks[idx])\n",
        "        elif self.imgs[idx].__contains__('bpop1000k'):\n",
        "          img_path = os.path.join(self.root,  \"/content/drive/My Drive/CS230/data/bpop1000k/unlabeled\", self.imgs[idx])\n",
        "          mask_path = os.path.join(self.root, \"/content/drive/My Drive/CS230/data/bpop1000k/masks\", self.masks[idx])\n",
        "        else:\n",
        "          img_path = os.path.join(self.root,  \"/content/drive/My Drive/CS230/data/bpopunlmtd/unlabeled\", self.imgs[idx])\n",
        "          mask_path = os.path.join(self.root, \"/content/drive/My Drive/CS230/data/bpopunlmtd/masks\", self.masks[idx])\n",
        "        \n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        \n",
        "  \n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        # mask = Image.open(mask_path)\n",
        "        #mask = np.array(mask)\n",
        "        #print(mask.shape)\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "        mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            test_area = (ymax-ymin) * (xmax-xmin)\n",
        "            if test_area > 0:\n",
        "              boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "        \n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIPrometI5Zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "      \n",
        "def get_instance_segmentation_model(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # now get the number of input features for the mask classifier\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # and replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctxr8XbyI7Dm",
        "colab_type": "code",
        "outputId": "426004ee-b0cf-4421-bcf1-186eb35ad01c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 8398 (delta 1), reused 1 (delta 0), pack-reused 8391\u001b[K\n",
            "Receiving objects: 100% (8398/8398), 10.26 MiB | 16.89 MiB/s, done.\n",
            "Resolving deltas: 100% (5778/5778), done.\n",
            "Note: checking out 'v0.3.0'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at be37608 version check against PyTorch's CUDA version\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2fTJdyoLEaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        # during training, randomly flip the training images\n",
        "        # and ground-truth for data augmentation\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fevSWwuyI_is",
        "colab_type": "code",
        "outputId": "71757f01-fe6f-4643-fef9-d55c7ca42e98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "89fa3ee5682e45c6b6defee40e4707a3",
            "9a27abdfe9e24f4895d1cff8a19e3987",
            "c73a8b430d6a432f8574f187704d9bed",
            "e045d0bee8aa4c6b8528b815ddb4ff34",
            "704f7ac3160349c4b9fec40b02111279",
            "dd9497bf1d8445eb8468e5f53d49e664",
            "649f4be9e3a944e3acfbed15b4c84a54",
            "fb25c12bae544d4399a81858e93c74c7"
          ]
        }
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# our dataset has two classes only - background and person\n",
        "num_classes = 2\n",
        "\n",
        "# get the model using our helper function\n",
        "model = get_instance_segmentation_model(num_classes)\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "#CHANGE WEIGHTS HERE\n",
        "# get pretrained weights\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/CS230/parameters/RCNN_v3 (west_eu)/paramsrcnnv3westeu_model2bxnonlogis900_epoch2.pth'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89fa3ee5682e45c6b6defee40e4707a3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=178090079.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1efXsx0KkM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_human_label = SatelliteDataset('/content/drive/My Drive/CS230/data', get_transform(train=False))\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset_human_label)).tolist()\n",
        "dataset_human_label = torch.utils.data.Subset(dataset_human_label, indices[-250:])\n",
        "num_dataset_human_label = len(dataset_human_label)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuRc77VLLMRR",
        "colab_type": "code",
        "outputId": "ec7a76c4-0a3e-483b-f195-5ac9428af83a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "dev_filenames = os.listdir('/content/drive/My Drive/CS230/data/self_labeled/label_suggestions/IoU@25_threshold@20/final_masks')\n",
        "\n",
        "num_dev = len(dev_filenames)\n",
        "\n",
        "acc_per_img = np.zeros((num_dev,1))\n",
        "recall_per_img = np.zeros((num_dev,1))\n",
        "\n",
        "final_area = 0\n",
        "overlap_area = 0\n",
        "prd_area = 0\n",
        "google_area = 0\n",
        "overlap_google_area = 0\n",
        "google_objects = 0\n",
        "final_objects = 0\n",
        "\n",
        "for k in range(num_dev):\n",
        "\n",
        "  temp_idx = int(dev_filenames[k].split('.',2)[0])\n",
        "  temp_img, temp_target = dataset_human_label[temp_idx]\n",
        "  temp_prd_mask_img = temp_img.mul(255).permute(1, 2, 0).byte().numpy()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      prediction = model([temp_img.to(device)])\n",
        "\n",
        "  #init image of predicted mask\n",
        "\n",
        "  temp_prd_mask_img[:,:,0]= 0\n",
        "  temp_prd_mask_img[:,:,1]= 0\n",
        "  temp_prd_mask_img[:,:,2]= 0\n",
        "\n",
        "  num_predicted_masks = len(prediction[0]['scores'])\n",
        "  prd_mask = np.zeros((640,640))\n",
        "  temp_prd_mask = np.zeros((640,640))\n",
        "\n",
        "  #add predicted masks\n",
        "  for i in range(0,num_predicted_masks):\n",
        "    np.random.seed(3) \n",
        "    r_channel = np.random.uniform(low=0, high=255, size=num_predicted_masks).astype(int)\n",
        "    np.random.seed(4) \n",
        "    g_channel = np.random.uniform(low=0, high=255, size=num_predicted_masks).astype(int)\n",
        "    np.random.seed(5) \n",
        "    b_channel = np.random.uniform(low=0, high=255, size=num_predicted_masks).astype(int)\n",
        "    if (prediction[0]['scores'][i] > 0.5):\n",
        "      temp_prd_mask = np.asarray(prediction[0]['masks'][i, 0].cpu())\n",
        "      temp_prd_mask[temp_prd_mask >= 0.5] = 1\n",
        "      temp_prd_mask[temp_prd_mask < 0.5] = 0\n",
        "      temp_prd_mask_img[:,:,0][temp_prd_mask==1] = r_channel[i]\n",
        "      temp_prd_mask_img[:,:,1][temp_prd_mask==1] = g_channel[i]\n",
        "      temp_prd_mask_img[:,:,2][temp_prd_mask==1] = b_channel[i]\n",
        "      prd_mask[temp_prd_mask == 1] = 1 \n",
        "      \n",
        "  final_mask_path = '/content/drive/My Drive/CS230/data/self_labeled/label_suggestions/IoU@25_threshold@20/final_masks/'+dev_filenames[k]\n",
        "  final_mask_img = Image.open(final_mask_path).convert(\"L\")\n",
        "  final_mask = np.array(final_mask_img)\n",
        "  final_objects += len(np.unique(final_mask))-1\n",
        "  final_mask[final_mask > 0 ] = 1\n",
        "\n",
        "\n",
        "  acc_per_img[k] = np.sum(prd_mask * final_mask) / np.sum(prd_mask)\n",
        "  recall_per_img[k]  = np.sum(prd_mask * final_mask) / np.sum(final_mask)\n",
        "  final_area += np.sum(final_mask)\n",
        "  prd_area += np.sum(prd_mask)\n",
        "  overlap_area += np.sum(prd_mask * final_mask)\n",
        "\n",
        "  google_mask = np.array(temp_target['masks'])\n",
        "  google_area += np.sum(google_mask)\n",
        "  overlap_google_area += np.sum(google_mask * final_mask)\n",
        "  google_objects += len(temp_target['labels'])\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:60: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:61: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjnHSnitVTw6",
        "colab_type": "code",
        "outputId": "2a1ce170-ad48-41be-f640-e6430534f20e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "overall_recall = overlap_area / final_area\n",
        "overall_acc = overlap_area / prd_area\n",
        "overall_f1 = 2*(overall_recall * overall_acc)/(overall_recall + overall_acc)\n",
        "print(overall_recall)\n",
        "print(overall_acc)\n",
        "print(overall_f1)\n",
        "print(final_area)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5977355144817518\n",
            "0.7568170141241489\n",
            "0.667934831248848\n",
            "7035682.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxg9lg1Jbk4E",
        "colab_type": "code",
        "outputId": "60be52f8-ff5f-4c7f-db03-b5a942882573",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "google_recall = overlap_google_area / final_area\n",
        "google_acc = overlap_google_area / google_area\n",
        "google_f1 = 2*(google_recall * google_acc)/(google_recall + google_acc)\n",
        "print(google_recall)\n",
        "print(google_acc)\n",
        "print(google_f1)\n",
        "print(google_area)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.29804416970522546\n",
            "0.8979865208495171\n",
            "0.44754645365988377\n",
            "2335162.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}